{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ed2dAh0Pu99cLAjN-_RVeMdTYOuU12OU",
      "authorship_tag": "ABX9TyNPjK9AULX38CEPHYJwSE11",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RounabhSahu/Blogging-App/blob/master/Attention_U_Net_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "roaU88hDX-GP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62af402e-7255-4197-d42c-f66cd8021586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import random\n",
        "import nibabel\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import nibabel as nib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from nibabel import load\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import Sequence\n",
        "from IPython.display import Image, display\n",
        "from skimage.exposure import rescale_intensity\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual,FloatRangeSlider,FloatSlider\n",
        "import ipywidgets as widgets\n",
        "\n",
        "from google.colab import drive\n",
        "from scipy.ndimage import zoom\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your data path\n",
        "data_path = '/content/drive/MyDrive/HYPODENSITY-DATA/'"
      ],
      "metadata": {
        "id": "nLY1WtOLKFx1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_files = sorted(glob(os.path.join(data_path, '*')))\n",
        "print(all_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSDtUaDMKicc",
        "outputId": "999c2a0f-9e72-4e1e-9051-df43d59d71f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/HYPODENSITY-DATA/ProxmedImg006', '/content/drive/MyDrive/HYPODENSITY-DATA/ProxmedImg013', '/content/drive/MyDrive/HYPODENSITY-DATA/ProxmedImg014', '/content/drive/MyDrive/HYPODENSITY-DATA/ProxmedImg021', '/content/drive/MyDrive/HYPODENSITY-DATA/ProxmedImg022', '/content/drive/MyDrive/HYPODENSITY-DATA/ProxmedImg025', '/content/drive/MyDrive/HYPODENSITY-DATA/ProxmedImg043', '/content/drive/MyDrive/HYPODENSITY-DATA/ProxmedImg331']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainHypoDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_filenames, mask_filenames, batch_size, image_size):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.mask_filenames = mask_filenames\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.length = int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.mask_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        x = np.zeros((self.batch_size, *self.image_size, 1))\n",
        "        y = np.zeros((self.batch_size, *self.image_size, 1))\n",
        "\n",
        "        for i, (image_filename, mask_filename) in enumerate(zip(batch_x, batch_y)):\n",
        "            image = nib.load(image_filename)\n",
        "            mask = nib.load(mask_filename)\n",
        "            # get the data from the image object\n",
        "            image_data = image.get_fdata()\n",
        "            mask_data = mask.get_fdata()\n",
        "            # get random slice from the volumes\n",
        "            slice_index = np.random.randint(0, image_data.shape[2] - 1)\n",
        "            x[i, :, :, 0] = image_data[:, :, slice_index]\n",
        "            y[i, :, :, 0] = (mask_data[:, :, slice_index] > 0).astype(np.float32)  # Assuming mask is binary\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# Get the list of train images and masks\n",
        "train_images = sorted(glob(os.path.join(data_path, '*', '*_NCCT.nii.gz')))\n",
        "train_masks = sorted(glob(os.path.join(data_path, '*', '*_ROI.nii.gz')))\n",
        "\n",
        "# Set batch size and image size\n",
        "batch_size = 1\n",
        "image_size = (512, 512)\n",
        "\n",
        "# Create data generators\n",
        "train_generator = BrainHypoDataGenerator(train_images[:10], train_masks[:10], batch_size, image_size)\n",
        "val_generator = BrainHypoDataGenerator(train_images[10:], train_masks[10:], batch_size, image_size)"
      ],
      "metadata": {
        "id": "OvEqLz9BX_5H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input,Conv1D,Conv2D, Conv3D, UpSampling3D, concatenate, Softmax,UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def attention_block(x, g, inter_channel):\n",
        "    theta = Conv2D(inter_channel, (1, 1), activation='relu', padding='same')(x)\n",
        "    phi = Conv2D(inter_channel, (1, 1), activation='relu', padding='same')(x/2)\n",
        "\n",
        "    # Upsample phi to match the dimensions of theta\n",
        "    phi = Conv2D(inter_channel, (1, 1), activation='relu', padding='same')(phi)\n",
        "\n",
        "    attn = Softmax(axis=-1)(theta * phi)\n",
        "    y = Conv2D(inter_channel, (1, 1), activation='relu', padding='same')(attn * x)\n",
        "    return y\n",
        "\n",
        "\n",
        "def convolution_block(x, filters, kernel_size, strides=(1, 1), activation='relu', padding='same'):\n",
        "    x = Conv2D(filters, kernel_size, strides=strides, activation=activation, padding=padding)(x)\n",
        "    return x\n",
        "\n",
        "def attention_unet_2d(input_shape, filters=64, attention_inter_channel=32):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = convolution_block(inputs, filters, (3, 3))\n",
        "    # No pooling in the encoder\n",
        "    conv2 = convolution_block(conv1, filters * 2, (3, 3))\n",
        "\n",
        "    # Attention Block\n",
        "    attention_g = UpSampling2D(size=(2, 2))(conv2)  # Upsample for matching dimensions\n",
        "    attention_block_2 = attention_block(conv2, attention_g, attention_inter_channel)\n",
        "\n",
        "    # Decoder\n",
        "    up1 = UpSampling2D(size=(2, 2))(conv2)\n",
        "    concat1 = concatenate([up1, attention_block_2], axis=-1)\n",
        "    conv3 = convolution_block(concat1, filters, (3, 3))\n",
        "\n",
        "    up2 = UpSampling2D(size=(2, 2))(conv3)\n",
        "    concat2 = concatenate([up2, conv1], axis=-1)\n",
        "    conv4 = convolution_block(concat2, filters, (3, 3))\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv4)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile the model and specify loss, optimizer, and metrics\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model with input shape\n",
        "input_shape = (*image_size, 1)\n",
        "model = attention_unet_2d(input_shape)\n",
        "\n",
        "# Train the model with your dataset\n",
        "model.fit(train_generator, epochs=epochs, validation_data=val_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "ZlADuQ0bIcGF",
        "outputId": "550eccc8-d1b1-4456-c28a-a7a329bc414e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-59606b53a245>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Create the model with input shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_unet_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Train the model with your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-59606b53a245>\u001b[0m in \u001b[0;36mattention_unet_2d\u001b[0;34m(input_shape, filters, attention_inter_channel)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Attention Block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mattention_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Upsample for matching dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mattention_block_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_inter_channel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-59606b53a245>\u001b[0m in \u001b[0;36mattention_block\u001b[0;34m(x, g, inter_channel)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minter_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/tf_op_layer.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         ):\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mTFOpLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"tf.math.multiply_14\" (type TFOpLambda).\n\nDimensions must be equal, but are 32 and 128 for '{{node tf.math.multiply_14/Mul}} = Mul[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [?,512,512,32], [?,512,512,128].\n\nCall arguments received by layer \"tf.math.multiply_14\" (type TFOpLambda):\n  • x=tf.Tensor(shape=(None, 512, 512, 32), dtype=float32)\n  • y=tf.Tensor(shape=(None, 512, 512, 128), dtype=float32)\n  • name=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9gluaRE7wzyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATOsqAf-QQWj",
        "outputId": "622cfbb6-4b2f-4f0e-9547-2345135611b9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 13s 1s/step - loss: 0.0553 - accuracy: 0.9896\n",
            "Test Loss: 0.05528793856501579\n",
            "Test Accuracy: 0.9895777702331543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbEIUV42YI65"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}